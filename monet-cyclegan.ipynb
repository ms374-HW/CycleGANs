{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Using CycleGAN for Image Translation\n\nUsing the CycleGAN implementation by https://hardikbansal.github.io/CycleGANBlog/\n\nFor more information, check out [TensorFlow](https://www.tensorflow.org/tutorials/generative/cyclegan) and [Keras](https://keras.io/examples/generative/cyclegan/) CycleGAN documentation pages.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\n\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport os\nimport time\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n    \nprint(tf.__version__)\ntf.compat.v1.enable_eager_execution()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T18:42:09.690095Z","iopub.execute_input":"2021-05-30T18:42:09.690703Z","iopub.status.idle":"2021-05-30T18:42:15.813275Z","shell.execute_reply.started":"2021-05-30T18:42:09.690666Z","shell.execute_reply":"2021-05-30T18:42:15.811857Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Number of replicas: 1\n2.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# LOADING THE FILE NAMES\n\nMONET_FILENAMES = tf.io.gfile.glob(str('../input/monet-gan-getting-started/monet_tfrec/*.tfrec'))\nprint('Monet TFRecord Files:', len(MONET_FILENAMES))\n\nPHOTO_FILENAMES = tf.io.gfile.glob(str('../input/monet-gan-getting-started/photo_tfrec/*.tfrec'))\nprint('Photo TFRecord Files:', len(PHOTO_FILENAMES))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T18:42:16.194095Z","iopub.execute_input":"2021-05-30T18:42:16.194508Z","iopub.status.idle":"2021-05-30T18:42:16.215284Z","shell.execute_reply.started":"2021-05-30T18:42:16.194475Z","shell.execute_reply":"2021-05-30T18:42:16.213713Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Monet TFRecord Files: 5\nPhoto TFRecord Files: 20\n","output_type":"stream"}]},{"cell_type":"code","source":"IMAGE_SIZE = [256, 256]  # given\n\ndef decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=3) # 3 channels = RGB, which is given\n    # normalise image in range [-1, 1]\n    image = (tf.cast(image, tf.float32) / 127.5) - 1\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\n\ndef read_tfrecord(example):\n    tfrecord_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    return image\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2021-05-30T18:42:18.921827Z","iopub.execute_input":"2021-05-30T18:42:18.922320Z","iopub.status.idle":"2021-05-30T18:42:18.935589Z","shell.execute_reply.started":"2021-05-30T18:42:18.922273Z","shell.execute_reply":"2021-05-30T18:42:18.934093Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# LOADING THE DATA IMAGES\nmonet_ds = load_dataset(MONET_FILENAMES, labeled=True).batch(1)\nphoto_ds = load_dataset(PHOTO_FILENAMES, labeled=True).batch(1)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T18:42:22.384013Z","iopub.execute_input":"2021-05-30T18:42:22.384599Z","iopub.status.idle":"2021-05-30T18:42:22.725114Z","shell.execute_reply.started":"2021-05-30T18:42:22.384562Z","shell.execute_reply":"2021-05-30T18:42:22.723979Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Generator\n3 Components used in the Generator\n1. Encoder - 3 Conv. Layers\n1. Transformer\n1. Decoder - 2 Deconv. layers + Conv. Layer","metadata":{}},{"cell_type":"markdown","source":"## Encoder\n* inputimg = input image [256, 256, 3]\n* num_features = number of filters in the first layer of generator\n* window width, window height = the sliding window dimensions that slides across the images\n* stride width, stride height = the shift by which the window slides with\n\n\n    def general_conv2d(inputc, o_d=64, f_h=7, f_w=7, s_h=1, s_w=1):\n\n    with tf.variable_scope(name):\n        conv = tf.contrib.layers.conv2d(inputc, num_features, [window_width, window_height], [stride_width, stride_height],\n                                        padding, activation_fn=None, weights_initializer=tf.truncated_normal_initializer(stddev=stddev),\n                                        biases_initializer=tf.constant_initializer(0.0))\n\nAs you can see, with more conv layers, the number of features also increases. This is to encode the features of an 256 x 256 image with only 3 features leaving only a 64 x 64 image with 256 features                                       ","metadata":{}},{"cell_type":"markdown","source":"## Transformation\nWe need to remember not to deviate too much from the original image, since the translation should still reserve some trace of the original image. Hence we transform using a resnet layer\n\nThe resnet layer consists of 2 conv layers and a function to add:\n1. The input goes through the first conv layer\n1. The tensor from the first layer goes to the second\n1. The second tensor is added to the original input and returned\n\n    def build_resnet_block(input_res, num_features):\n\n        out_res_1 = general_conv2d(input_res, num_features,\n                                   window_width=3,\n                                   window_heigth=3,\n                                   stride_width=1,\n                                   stride_heigth=1)\n        out_res_2 = general_conv2d(out_res_1, num_features,\n                                   window_width=3,\n                                   window_heigth=3,\n                                   stride_width=1,\n                                   stride_heigth=1)\n        return (out_res_2 + input_res)","metadata":{}},{"cell_type":"markdown","source":"## Decoding \nNow we have to return the tensor into a viable image. Hence, the tensor needs to unpack from a [64, 64, 256] into a [256, 256, 3] image","metadata":{}},{"cell_type":"markdown","source":"## Final Generator","metadata":{}},{"cell_type":"code","source":"# important numbers\ngen_filter = 64 # Number of filters in first layer of generator\ndis_filter = 64 # Number of filters in first layer of discriminator\nbatch_size = 1 # batch_size\npool_size = 50 # pool_size\nimg_width = 256 # Imput image will of width 256\nimg_height = 256 # Input image will be of height 256\nchannels = 3 # RGB format","metadata":{"execution":{"iopub.status.busy":"2021-05-30T18:42:30.685750Z","iopub.execute_input":"2021-05-30T18:42:30.686173Z","iopub.status.idle":"2021-05-30T18:42:30.692158Z","shell.execute_reply.started":"2021-05-30T18:42:30.686126Z","shell.execute_reply":"2021-05-30T18:42:30.690937Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def build_resnet_block(input_res, num_features):\n\n    out_res_1 = general_conv2d(input_res, num_features,\n                               window_width=3,\n                               window_height=3,\n                               stride_width=1,\n                               stride_height=1)\n    out_res_2 = general_conv2d(out_res_1, num_features,\n                               window_width=3,\n                               window_height=3,\n                               stride_width=1,\n                               stride_height=1)\n    return (out_res_2 + input_res)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T18:42:33.608885Z","iopub.execute_input":"2021-05-30T18:42:33.609310Z","iopub.status.idle":"2021-05-30T18:42:33.616844Z","shell.execute_reply.started":"2021-05-30T18:42:33.609273Z","shell.execute_reply":"2021-05-30T18:42:33.615628Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"tf.config.experimental_run_functions_eagerly(True)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:24:22.032800Z","iopub.execute_input":"2021-05-30T17:24:22.033201Z","iopub.status.idle":"2021-05-30T17:24:22.038349Z","shell.execute_reply.started":"2021-05-30T17:24:22.033163Z","shell.execute_reply":"2021-05-30T17:24:22.037223Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def general_conv2d(inputconv, num_features=64, window_height=7, window_width=7, stride_height=1, stride_width=1, stddev=0.02, padding=None, name=\"conv2d\", do_norm=True, do_relu=True):\n    s = inputconv.shape[3]\n    with tf.compat.v1.Session() as sess:\n        with tf.compat.v1.variable_scope(name, reuse=tf.compat.v1.AUTO_REUSE):\n\n            w = tf.Variable(tf.compat.v1.truncated_normal([window_height, window_width, s, num_features], stddev=0.5))      \n            conv = tf.nn.conv2d(inputconv, filters=w, strides=[1,stride_width,stride_height,1] , padding='SAME')\n#             conv = tf.contrib.layers.conv2d(inputconv, num_features, window_height, stride_height, padding, activation_fn=None, weights_initializer=tf.compat.v1.truncated_normal_initializer(stddev=stddev),biases_initializer=tf.constant_initializer(0.0))\n            biases = tf.compat.v1.get_variable('b_'+str(num_features),[num_features],initializer=tf.constant_initializer(0.0))\n            conv = tf.nn.bias_add(conv,biases)\n            if do_norm:\n                dims = conv.shape\n                scale = tf.compat.v1.get_variable('scale_'+str(dims[3]-dims[1])+'_'+str(num_features),shape=[dims[1],dims[2],dims[3]],initializer=tf.constant_initializer(1))\n                beta = tf.compat.v1.get_variable('beta_'+str(dims[3]-dims[1])+'_'+str(num_features),shape=[dims[1],dims[2],dims[3]],initializer=tf.constant_initializer(0))\n                conv_mean,conv_var = tf.nn.moments(conv,[0])\n                conv = tf.nn.batch_normalization(conv,conv_mean,conv_var,beta,scale,0.001)\n            if do_relu:\n                conv = tf.nn.relu(conv)\n    return conv","metadata":{"execution":{"iopub.status.busy":"2021-05-30T18:44:56.994186Z","iopub.execute_input":"2021-05-30T18:44:56.994599Z","iopub.status.idle":"2021-05-30T18:44:57.012847Z","shell.execute_reply.started":"2021-05-30T18:44:56.994564Z","shell.execute_reply":"2021-05-30T18:44:57.011656Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def build_generator(input_gen):\n    # Encoding \n    \n    # the first Conv layer, with the input info\n    # returns a tensor with shape = [256, 256, 64]\n    o_c1 = general_conv2d(input_gen, num_features=gen_filter, window_width=7, window_height=7, stride_width=1, stride_height=1)\n    # the second Conv layer, with the previous tensor as input\n    # returns a tensor with shape = [128, 128, 128]\n    o_c2 = general_conv2d(o_c1, num_features=gen_filter*2, window_width=3, window_height=3, stride_width=2, stride_height=2)\n    # the third Conv layer, with the previous tensor as input\n    # returns a tensor with shape = [64, 64, 256]\n    o_enc_A = general_conv2d(o_c2, num_features=gen_filter*4, window_width=3, window_height=3, stride_width=2, stride_height=2)\n\n    # Transformation\n    \n    # input is the last tensor from the generator\n    o_r1 = build_resnet_block(o_enc_A, num_features=64*4)\n    # this chain goes on for a total of 6 resnet layers\n    o_r2 = build_resnet_block(o_r1, num_features=64*4)\n    o_r3 = build_resnet_block(o_r2, num_features=64*4)\n    o_r4 = build_resnet_block(o_r3, num_features=64*4)\n    o_r5 = build_resnet_block(o_r4, num_features=64*4)\n    # the shape of the final tensor is reserved to be [64, 64, 256]\n    o_enc_B = build_resnet_block(o_r5, num_features=64*4)\n\n    #Decoding\n    o_d1 = general_conv2d(o_enc_B, num_features=gen_filter*2, window_width=3, window_height=3, stride_width=2, stride_height=2)\n    o_d2 = general_conv2d(o_d1, num_features=gen_filter, window_width=3, window_height=3, stride_width=2, stride_height=2)\n    gen_B = general_conv2d(o_d2, num_features=3, window_width=7, window_height=7, stride_width=1, stride_height=1)\n\n    return gen_B","metadata":{"execution":{"iopub.status.busy":"2021-05-30T18:42:44.539126Z","iopub.execute_input":"2021-05-30T18:42:44.539544Z","iopub.status.idle":"2021-05-30T18:42:44.554617Z","shell.execute_reply.started":"2021-05-30T18:42:44.539510Z","shell.execute_reply":"2021-05-30T18:42:44.553570Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Discriminator\nHas multiple Convolutional Layers","metadata":{}},{"cell_type":"code","source":"def build_discriminator(input_disc):\n    o_c1 = general_conv2d(input_disc, dis_filter, 7, 7, 2, 2)\n    o_c2 = general_conv2d(o_c1, dis_filter*2, 7, 7, 2, 2)\n    o_enc_A = general_conv2d(o_c2, dis_filter*4, 7, 7, 2, 2)\n    o_c4 = general_conv2d(o_enc_A, dis_filter*8, 7, 7, 2, 2)\n\n    # making decision\n    decision = general_conv2d(o_c4, 1, 7, 7, 1, 1, 0.02)\n    return decision","metadata":{"execution":{"iopub.status.busy":"2021-05-30T18:42:48.657379Z","iopub.execute_input":"2021-05-30T18:42:48.657841Z","iopub.status.idle":"2021-05-30T18:42:48.665347Z","shell.execute_reply.started":"2021-05-30T18:42:48.657796Z","shell.execute_reply":"2021-05-30T18:42:48.664199Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"tf.compat.v1.disable_eager_execution()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T18:42:53.177172Z","iopub.execute_input":"2021-05-30T18:42:53.177834Z","iopub.status.idle":"2021-05-30T18:42:53.182269Z","shell.execute_reply.started":"2021-05-30T18:42:53.177789Z","shell.execute_reply":"2021-05-30T18:42:53.181169Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"lr = 0.0002\ndef train():\n    input_A = tf.compat.v1.placeholder(tf.float32, [1, 256, 256, 3], name=\"input_A\")\n    input_B = tf.compat.v1.placeholder(tf.float32, [1, 256, 256, 3], name=\"input_B\")\n\n    fake_pool_A = tf.compat.v1.placeholder(tf.float32, [None, 256, 256, 3], name=\"fake_pool_A\")\n    fake_pool_B = tf.compat.v1.placeholder(tf.float32, [None, 256, 256, 3], name=\"fake_pool_B\")\n\n    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n    num_fake_inputs = 0\n    \n    with tf.compat.v1.variable_scope(\"Model\") as scope:\n        fake_B = build_generator(input_A)\n        fake_A = build_generator(input_B)\n        rec_A = build_discriminator(input_A)\n        rec_B = build_discriminator(input_B)\n        scope.reuse_variables()\n        fake_rec_A = build_discriminator(fake_A)\n        fake_rec_B = build_discriminator(fake_B)\n        cyc_A = build_generator(fake_B)\n        cyc_B = build_generator(fake_A)\n        scope.reuse_variables()\n        fake_pool_rec_A = build_discriminator(fake_pool_A)\n        fake_pool_rec_B = build_discriminator(fake_pool_B)\n        \n    # Loss functions\n    \n    # cyclic loss function to not lose the original input too much\n    cyc_loss = tf.reduce_mean(tf.abs(input_A-cyc_A)) + tf.reduce_mean(tf.abs(input_B-cyc_B))\n    \n    # loss for the discriminator\n    d_loss_A1 = tf.reduce_mean(tf.compat.v1.squared_difference(fake_rec_A,1))\n    d_loss_B1 = tf.reduce_mean(tf.compat.v1.squared_difference(fake_rec_B,1))\n    \n    # loss for the generator\n    g_loss_A = cyc_loss*10 + d_loss_B1\n    g_loss_B = cyc_loss*10 + d_loss_A1\n    \n    d_loss_A = (tf.reduce_mean(tf.compat.v1.square(fake_pool_rec_A)) + tf.reduce_mean(tf.compat.v1.squared_difference(rec_A,1)))/2.0\n    d_loss_B = (tf.reduce_mean(tf.compat.v1.square(fake_pool_rec_B)) + tf.reduce_mean(tf.compat.v1.squared_difference(rec_B,1)))/2.0\n    \n    lr = tf.compat.v1.placeholder(tf.float32, shape=[], name=\"lr\")\n    \n    optimizer = tf.compat.v1.train.AdamOptimizer(lr)\n    \n    model_vars = tf.compat.v1.trainable_variables()\n    \n    d_A_vars = [var for var in model_vars if 'd_A' in var.name]\n    g_A_vars = [var for var in model_vars if 'g_A' in var.name]\n    d_B_vars = [var for var in model_vars if 'd_B' in var.name]\n    g_B_vars = [var for var in model_vars if 'g_B' in var.name]\n    \n    # optimising \n    d_A_trainer = optimizer.minimize(d_loss_A, var_list=d_A_vars)\n    d_B_trainer = optimizer.minimize(d_loss_B, var_list=d_B_vars)\n    g_A_trainer = optimizer.minimize(g_loss_A, var_list=g_A_vars)\n    g_B_trainer = optimizer.minimize(g_loss_B, var_list=g_B_vars)\n    \n    for var in model_vars: print(var.name)\n        \n    # Summary Variables\n    g_A_loss_summ = tf.summary.scalar(\"g_A_loss\", g_loss_A)\n    g_B_loss_summ = tf.summary.scalar(\"g_B_loss\", g_loss_B)\n    d_A_loss_summ = tf.summary.scalar(\"d_A_loss\", d_loss_A)\n    d_B_loss_summ = tf.summary.scalar(\"d_B_loss\", d_loss_B)\n    \n    init = tf.global_variables_initializer()\n    saver = tf.train.Saver()\n    \n\n        \n#         coord = tf.train.Coordinator()\n#         threads = tf.train.start_queue_runners(coord=coord)\n#         num_monets = sess.run(queue_length_A)\n#         num_photos= sess.run(queue_length_B)\n#         images_A = []\n#         images_B = []\n        \n#         # 10 here is num of max images\n#         A_input = np.zeros((10,1,256, 256, 3))\n#         B_input = np.zeros((10,1,256, 256, 3))\n        \n#         for i in range(10): \n#             image_tensor = sess.run(monet_ds)\n#             A_input[i] = image_tensor.reshape((1,256, 256, 3))\n            \n#         for i in range(10):\n#             image_tensor = sess.run(photos_ds)\n#             B_input[i] = image_tensor.reshape((1,256, 256, 3))\n            \n#         coord.request_stop()\n#         coord.join(threads)\n        \n#         for ptr in range(0,100):\n#             fake_A_temp, fake_B_temp, cyc_A_temp, cyc_B_temp = sess.run([fake_A, fake_B, cyc_A, cyc_B],feed_dict={input_A:A_input[0], input_B:B_input[0]})\n#             imsave(\"./output/fakeB_\"+str(ptr)+\".jpg\",((fake_A_temp[0]+1)*127.5).astype(np.uint8))\n#             imsave(\"./output/fakeA_\"+str(ptr)+\".jpg\",((fake_B_temp[0]+1)*127.5).astype(np.uint8))\n#             imsave(\"./output/cycA_\"+str(ptr)+\".jpg\",((cyc_A_temp[0]+1)*127.5).astype(np.uint8))\n#             imsave(\"./output/cycB_\"+str(ptr)+\".jpg\",((cyc_B_temp[0]+1)*127.5).astype(np.uint8))\n#             imsave(\"./output/inputA_\"+str(ptr)+\".jpg\",((A_input[0][0]+1)*127.5).astype(np.uint8))\n#             imsave(\"./output/inputB_\"+str(ptr)+\".jpg\",((B_input[0][0]+1)*127.5).astype(np.uint8))\n        \n        \n        # Loading images into the tensors\n    with tf.Session() as sess:\n        sess.run(init)\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(coord=coord)\n\n        num_files_A = sess.run(queue_length_A)\n        num_files_B = sess.run(queue_length_B)\n\n        images_A = []\n        images_B = []\n\n        # 50 is pool size\n        fake_images_A = np.zeros((50,1,256, 256, 3))\n        fake_images_B = np.zeros((50,1,256, 256, 3))\n\n        A_input = np.zeros((10,1,256, 256, 3))\n        B_input = np.zeros((10,1,256, 256, 3))\n\n        for i in range(10): \n            image_tensor = sess.run(monet_ds)\n            A_input[i] = image_tensor.reshape((1,256, 256, 3))\n\n        for i in range(10):\n            image_tensor = sess.run(photos_ds)\n            B_input[i] = image_tensor.reshape((1,256, 256, 3))\n\n        coord.request_stop()\n        coord.join(threads)\n\n        writer = tf.summary.FileWriter(\"./output/2\")\n        check_dir = \"./output/checkpoints/\"\n\n        for epoch in range(sess.run(global_step),1):\n            print (\"In the epoch \", epoch)\n            saver.save(sess,os.path.join(check_dir,\"cyclegan\"),global_step=epoch)\n\n            if(epoch < 100) :\n                curr_lr = 0.0002\n            else:\n                curr_lr = 0.0002 - 0.0002*(epoch-100)/100\n\n            summary_str, cyc_A_temp = sess.run([summary_op, cyc_A],feed_dict={input_A:A_input[0], input_B:B_input[0]})\n            imsave(\"./output/output_\"+str(epoch)+\".jpg\",((cyc_A_temp[0]+1)*127.5).astype(np.uint8))\n            imsave(\"./output/input.jpg\",((A_input[0][0]+1)*127.5).astype(np.uint8))\n\n            writer.add_summary(summary_str, epoch)\n            \n        for ptr in range(0,max_images):\n            print(\"In the iteration \",ptr)\n            print(\"Starting\",time.time()*1000.0)\n            \n            # Optimizing the G_A network\n            _, fake_B_temp, summary_str = sess.run([g_A_trainer, fake_B, g_A_loss_summ],feed_dict={input_A:A_input[ptr], input_B:B_input[ptr], lr:curr_lr})\n            writer.add_summary(summary_str, epoch*max_images + ptr)\n            print(\"After gA\", time.time()*1000.0)\n\n            fake_B_temp1 = fake_image_pool(num_fake_inputs, fake_B_temp, fake_images_B)\n\n            # Optimizing the D_B network\n            \n            _, summary_str = sess.run([d_B_trainer, d_B_loss_summ],feed_dict={input_A:A_input[ptr], input_B:B_input[ptr], lr:curr_lr, fake_pool_B:fake_B_temp1})\n            writer.add_summary(summary_str, epoch*max_images + ptr)\n            print(\"After dB\", time.time()*1000.0)\n\n            # Optimizing the G_B network\n            \n            _, fake_A_temp, summary_str = sess.run([g_B_trainer, fake_A, g_B_loss_summ],feed_dict={input_A:A_input[ptr], input_B:B_input[ptr], lr:curr_lr})\n            writer.add_summary(summary_str, epoch*max_images + ptr)\n            print(\"After gB\", time.time()*1000.0)\n\n            fake_A_temp1 = fake_image_pool(num_fake_inputs, fake_A_temp, fake_images_A)\n            \n            # Optimizing the D_A network\n            \n            _, summary_str = sess.run([d_A_trainer, d_A_loss_summ],feed_dict={input_A:A_input[ptr], input_B:B_input[ptr], lr:curr_lr, fake_pool_A:fake_A_temp1})\n            writer.add_summary(summary_str, epoch*max_images + ptr)\n            print(\"After dA\", time.time()*1000.0)\n            num_fake_inputs+=1\n            \n            writer.add_summary(summary_str, epoch*max_images + ptr)\n            print(\"After dA\", time.time()*1000.0)\n            num_fake_inputs+=1\n            \n                        \n        sess.run(tf.assign(global_step, epoch + 1))\n    writer.add_graph(sess.graph)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-30T19:04:23.713642Z","iopub.execute_input":"2021-05-30T19:04:23.714051Z","iopub.status.idle":"2021-05-30T19:04:23.769710Z","shell.execute_reply.started":"2021-05-30T19:04:23.714016Z","shell.execute_reply":"2021-05-30T19:04:23.768315Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"tf.executing_eagerly()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:24:33.603489Z","iopub.execute_input":"2021-05-30T17:24:33.603917Z","iopub.status.idle":"2021-05-30T17:24:33.610136Z","shell.execute_reply.started":"2021-05-30T17:24:33.603881Z","shell.execute_reply":"2021-05-30T17:24:33.609079Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"train()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T19:04:27.080124Z","iopub.execute_input":"2021-05-30T19:04:27.080576Z","iopub.status.idle":"2021-05-30T19:04:28.962328Z","shell.execute_reply.started":"2021-05-30T19:04:27.080537Z","shell.execute_reply":"2021-05-30T19:04:28.960947Z"},"trusted":true},"execution_count":35,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-34-28b3faa7412b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# optimising\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0md_A_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_A_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0md_B_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_B_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mg_A_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_loss_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg_A_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0mprocessors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No variables to optimize.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m     \u001b[0mvar_refs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     grads = gradients.gradients(\n","\u001b[0;31mValueError\u001b[0m: No variables to optimize."],"ename":"ValueError","evalue":"No variables to optimize.","output_type":"error"}]},{"cell_type":"code","source":"input_A = tf.compat.v1.placeholder(tf.float32, [batch_size, img_width, img_height, img_layer], name=\"input_A\")\ninput_B = tf.compat.v1.placeholder(tf.float32, [batch_size, img_width, img_height, img_layer], name=\"input_B\")","metadata":{"execution":{"iopub.status.busy":"2021-05-28T17:31:03.444038Z","iopub.execute_input":"2021-05-28T17:31:03.44437Z","iopub.status.idle":"2021-05-28T17:31:03.464143Z","shell.execute_reply.started":"2021-05-28T17:31:03.444344Z","shell.execute_reply":"2021-05-28T17:31:03.463203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(input_A, input_B):\n    gen_B = build_generator(input_A, name=\"generator_AtoB\")\n    gen_A = build_generator(input_B, name=\"generator_BtoA\")\n    dec_A = build_discriminator(input_A, name=\"discriminator_A\")\n    dec_B = build_discriminator(input_B, name=\"discriminator_B\")\n\n    dec_gen_A = build_discriminator(gen_A, \"discriminator_A\")\n    dec_gen_B = build_discriminator(gen_B, \"discriminator_B\")\n    cyc_A = build_generator(gen_B, \"generator_BtoA\")\n    cyc_B = build_generator(gen_A, \"generator_AtoB\")\n    \n    \n    \n    d_A_trainer = optimizer.minimize(d_loss_A, var_list=d_A_vars)\n    d_B_trainer = optimizer.minimize(d_loss_B, var_list=d_B_vars)\n    g_A_trainer = optimizer.minimize(g_loss_A, var_list=g_A_vars)\n    g_B_trainer = optimizer.minimize(g_loss_B, var_list=g_B_vars)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-28T14:00:23.344188Z","iopub.execute_input":"2021-05-28T14:00:23.344523Z","iopub.status.idle":"2021-05-28T14:00:23.353109Z","shell.execute_reply.started":"2021-05-28T14:00:23.344495Z","shell.execute_reply":"2021-05-28T14:00:23.352298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(0,100):\n    # Define the learning rate schedule. The learning rate is kept\n    # constant upto 100 epochs and then slowly decayed\n    if(epoch < 100) :\n        curr_lr = 0.0002\n    else:\n        curr_lr = 0.0002 - 0.0002*(epoch-100)/100\n\n    # Running the training loop for all batches\n    for ptr in range(0,num_images):\n\n        # Train generator G_A->B\n        _, gen_B_temp = sess.run([g_A_trainer, gen_B],\n                                 feed_dict={input_A:A_input[ptr], input_B:B_input[ptr], lr:curr_lr})\n\n        # We need gen_B_temp because to calculate the error in training D_B\n        _ = sess.run([d_B_trainer],\n                     feed_dict={input_A:A_input[ptr], input_B:B_input[ptr], lr:curr_lr})\n\n        # Same for G_B->A and D_A as follow\n        _, gen_A_temp = sess.run([g_B_trainer, gen_A],\n                                 feed_dict={input_A:A_input[ptr], input_B:B_input[ptr], lr:curr_lr})\n        _ = sess.run([d_A_trainer],\n                     feed_dict={input_A:A_input[ptr], input_B:B_input[ptr], lr:curr_lr})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def random_crop(image):\n  cropped_image = tf.image.random_crop(\n      image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n\n  return cropped_image\n\n# upsizing and then cropping the image randomly\ndef random_jitter(image):\n  # resizing to 286 x 286 x 3\n  image = tf.image.resize(image, [286, 286],\n                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n  # randomly cropping to 256 x 256 x 3\n  image = random_crop(image)\n\n  # random mirroring\n  image = tf.image.random_flip_left_right(image)\n\n  return image","metadata":{"execution":{"iopub.status.busy":"2021-05-28T14:11:08.986397Z","iopub.execute_input":"2021-05-28T14:11:08.986782Z","iopub.status.idle":"2021-05-28T14:11:08.995704Z","shell.execute_reply.started":"2021-05-28T14:11:08.98675Z","shell.execute_reply":"2021-05-28T14:11:08.994585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_image_train(image, label):\n  image = random_jitter(image)\n  return image","metadata":{"execution":{"iopub.status.busy":"2021-05-28T14:11:49.728735Z","iopub.execute_input":"2021-05-28T14:11:49.729045Z","iopub.status.idle":"2021-05-28T14:11:49.735349Z","shell.execute_reply.started":"2021-05-28T14:11:49.729016Z","shell.execute_reply":"2021-05-28T14:11:49.733715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-05-28T14:00:38.866984Z","iopub.execute_input":"2021-05-28T14:00:38.867404Z","iopub.status.idle":"2021-05-28T14:00:39.506271Z","shell.execute_reply.started":"2021-05-28T14:00:38.867376Z","shell.execute_reply":"2021-05-28T14:00:39.505144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to view an example\nexample_monet = next(iter(monet_ds))\nexample_photo = next(iter(photo_ds))\n\nplt.subplot(121)\nplt.title('Photo')\nplt.imshow(example_photo[0] * 0.5 + 0.5)\n\nplt.subplot(122)\nplt.title('Monet')\nplt.imshow(example_monet[0] * 0.5 + 0.5)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T14:01:14.660291Z","iopub.execute_input":"2021-05-28T14:01:14.660724Z","iopub.status.idle":"2021-05-28T14:01:15.007484Z","shell.execute_reply.started":"2021-05-28T14:01:14.66069Z","shell.execute_reply":"2021-05-28T14:01:15.005369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build the generator\n\nWe'll be using a UNET architecture for our CycleGAN. To build our generator, let's first define our `downsample` and `upsample` methods.\n\nThe `downsample`, as the name suggests, reduces the 2D dimensions, the width and height, of the image by the stride. The stride is the length of the step the filter takes. Since the stride is 2, the filter is applied to every other pixel, hence reducing the weight and height by 2.\n\nWe'll be using an instance normalization instead of batch normalization. As the instance normalization is not standard in the TensorFlow API, we'll use the layer from TensorFlow Add-ons.","metadata":{}},{"cell_type":"code","source":"OUTPUT_CHANNELS = 3\n\ndef downsample(filters, size, apply_instancenorm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    result = keras.Sequential()\n    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n                             kernel_initializer=initializer, use_bias=False))\n\n    if apply_instancenorm:\n        result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n\n    result.add(layers.LeakyReLU())\n\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`Upsample` does the opposite of downsample and increases the dimensions of the of the image. `Conv2DTranspose` does basically the opposite of a `Conv2D` layer.","metadata":{}},{"cell_type":"code","source":"def upsample(filters, size, apply_dropout=False):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    result = keras.Sequential()\n    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n                                      padding='same',\n                                      kernel_initializer=initializer,\n                                      use_bias=False))\n\n    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n\n    if apply_dropout:\n        result.add(layers.Dropout(0.5))\n\n    result.add(layers.ReLU())\n\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's build our generator!\n\nThe generator first downsamples the input image and then upsample while establishing long skip connections. Skip connections are a way to help bypass the vanishing gradient problem by concatenating the output of a layer to multiple layers instead of only one. Here we concatenate the output of the downsample layer to the upsample layer in a symmetrical fashion.","metadata":{}},{"cell_type":"code","source":"def Generator():\n    inputs = layers.Input(shape=[256,256,3])\n\n    # bs = batch size\n    down_stack = [\n        downsample(64, 4, apply_instancenorm=False), # (bs, 128, 128, 64)\n        downsample(128, 4), # (bs, 64, 64, 128)\n        downsample(256, 4), # (bs, 32, 32, 256)\n        downsample(512, 4), # (bs, 16, 16, 512)\n        downsample(512, 4), # (bs, 8, 8, 512)\n        downsample(512, 4), # (bs, 4, 4, 512)\n        downsample(512, 4), # (bs, 2, 2, 512)\n        downsample(512, 4), # (bs, 1, 1, 512)\n    ]\n\n    up_stack = [\n        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n        upsample(512, 4), # (bs, 16, 16, 1024)\n        upsample(256, 4), # (bs, 32, 32, 512)\n        upsample(128, 4), # (bs, 64, 64, 256)\n        upsample(64, 4), # (bs, 128, 128, 128)\n    ]\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n                                  strides=2,\n                                  padding='same',\n                                  kernel_initializer=initializer,\n                                  activation='tanh') # (bs, 256, 256, 3)\n\n    x = inputs\n\n    # Downsampling through the model\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n\n    skips = reversed(skips[:-1])\n\n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = layers.Concatenate()([x, skip])\n\n    x = last(x)\n\n    return keras.Model(inputs=inputs, outputs=x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build the discriminator\n\nThe discriminator takes in the input image and classifies it as real or fake (generated). Instead of outputing a single node, the discriminator outputs a smaller 2D image with higher pixel values indicating a real classification and lower values indicating a fake classification.","metadata":{}},{"cell_type":"code","source":"def Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    inp = layers.Input(shape=[256, 256, 3], name='input_image')\n\n    x = inp\n\n    down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n\n    zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n    conv = layers.Conv2D(512, 4, strides=1,\n                         kernel_initializer=initializer,\n                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n\n    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n\n    leaky_relu = layers.LeakyReLU()(norm1)\n\n    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n\n    last = layers.Conv2D(1, 4, strides=1,\n                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n\n    return tf.keras.Model(inputs=inp, outputs=last)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    monet_generator = Generator() # transforms photos to Monet-esque paintings\n    photo_generator = Generator() # transforms Monet paintings to be more like photos\n\n    monet_discriminator = Discriminator() # differentiates real Monet paintings and generated Monet paintings\n    photo_discriminator = Discriminator() # differentiates real photos and generated photos","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since our generators are not trained yet, the generated Monet-esque photo does not show what is expected at this point.","metadata":{}},{"cell_type":"code","source":"to_monet = monet_generator(example_photo)\n\nplt.subplot(1, 2, 1)\nplt.title(\"Original Photo\")\nplt.imshow(example_photo[0] * 0.5 + 0.5)\n\nplt.subplot(1, 2, 2)\nplt.title(\"Monet-esque Photo\")\nplt.imshow(to_monet[0] * 0.5 + 0.5)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build the CycleGAN model\n\nWe will subclass a `tf.keras.Model` so that we can run `fit()` later to train our model. During the training step, the model transforms a photo to a Monet painting and then back to a photo. The difference between the original photo and the twice-transformed photo is the cycle-consistency loss. We want the original photo and the twice-transformed photo to be similar to one another.\n\nThe losses are defined in the next section.","metadata":{}},{"cell_type":"code","source":"class CycleGan(keras.Model):\n    def __init__(\n        self,\n        monet_generator,\n        photo_generator,\n        monet_discriminator,\n        photo_discriminator,\n        lambda_cycle=10,\n    ):\n        super(CycleGan, self).__init__()\n        self.m_gen = monet_generator\n        self.p_gen = photo_generator\n        self.m_disc = monet_discriminator\n        self.p_disc = photo_discriminator\n        self.lambda_cycle = lambda_cycle\n        \n    def compile(\n        self,\n        m_gen_optimizer,\n        p_gen_optimizer,\n        m_disc_optimizer,\n        p_disc_optimizer,\n        gen_loss_fn,\n        disc_loss_fn,\n        cycle_loss_fn,\n        identity_loss_fn\n    ):\n        super(CycleGan, self).compile()\n        self.m_gen_optimizer = m_gen_optimizer\n        self.p_gen_optimizer = p_gen_optimizer\n        self.m_disc_optimizer = m_disc_optimizer\n        self.p_disc_optimizer = p_disc_optimizer\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n        \n    def train_step(self, batch_data):\n        real_monet, real_photo = batch_data\n        \n        with tf.GradientTape(persistent=True) as tape:\n            # photo to monet back to photo\n            fake_monet = self.m_gen(real_photo, training=True)\n            cycled_photo = self.p_gen(fake_monet, training=True)\n\n            # monet to photo back to monet\n            fake_photo = self.p_gen(real_monet, training=True)\n            cycled_monet = self.m_gen(fake_photo, training=True)\n\n            # generating itself\n            same_monet = self.m_gen(real_monet, training=True)\n            same_photo = self.p_gen(real_photo, training=True)\n\n            # discriminator used to check, inputing real images\n            disc_real_monet = self.m_disc(real_monet, training=True)\n            disc_real_photo = self.p_disc(real_photo, training=True)\n\n            # discriminator used to check, inputing fake images\n            disc_fake_monet = self.m_disc(fake_monet, training=True)\n            disc_fake_photo = self.p_disc(fake_photo, training=True)\n\n            # evaluates generator loss\n            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n\n            # evaluates total cycle consistency loss\n            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n\n            # evaluates total generator loss\n            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)\n            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n\n            # evaluates discriminator loss\n            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n\n        # Calculate the gradients for generator and discriminator\n        monet_generator_gradients = tape.gradient(total_monet_gen_loss,\n                                                  self.m_gen.trainable_variables)\n        photo_generator_gradients = tape.gradient(total_photo_gen_loss,\n                                                  self.p_gen.trainable_variables)\n\n        monet_discriminator_gradients = tape.gradient(monet_disc_loss,\n                                                      self.m_disc.trainable_variables)\n        photo_discriminator_gradients = tape.gradient(photo_disc_loss,\n                                                      self.p_disc.trainable_variables)\n\n        # Apply the gradients to the optimizer\n        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients,\n                                                 self.m_gen.trainable_variables))\n\n        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients,\n                                                 self.p_gen.trainable_variables))\n\n        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients,\n                                                  self.m_disc.trainable_variables))\n\n        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,\n                                                  self.p_disc.trainable_variables))\n        \n        return {\n            \"monet_gen_loss\": total_monet_gen_loss,\n            \"photo_gen_loss\": total_photo_gen_loss,\n            \"monet_disc_loss\": monet_disc_loss,\n            \"photo_disc_loss\": photo_disc_loss\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define loss functions\n\nThe discriminator loss function below compares real images to a matrix of 1s and fake images to a matrix of 0s. The perfect discriminator will output all 1s for real images and all 0s for fake images. The discriminator loss outputs the average of the real and generated loss.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    def discriminator_loss(real, generated):\n        real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n\n        generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n\n        total_disc_loss = real_loss + generated_loss\n\n        return total_disc_loss * 0.5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The generator wants to fool the discriminator into thinking the generated image is real. The perfect generator will have the discriminator output only 1s. Thus, it compares the generated image to a matrix of 1s to find the loss.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    def generator_loss(generated):\n        return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We want our original photo and the twice transformed photo to be similar to one another. Thus, we can calculate the cycle consistency loss be finding the average of their difference.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n        loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n\n        return LAMBDA * loss1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The identity loss compares the image with its generator (i.e. photo with photo generator). If given a photo as input, we want it to generate the same image as the image was originally a photo. The identity loss compares the input with the output of the generator.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    def identity_loss(real_image, same_image, LAMBDA):\n        loss = tf.reduce_mean(tf.abs(real_image - same_image))\n        return LAMBDA * 0.5 * loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the CycleGAN\n\nLet's compile our model. Since we used `tf.keras.Model` to build our CycleGAN, we can just ude the `fit` function to train our model.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n    monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    cycle_gan_model = CycleGan(\n        monet_generator, photo_generator, monet_discriminator, photo_discriminator\n    )\n\n    cycle_gan_model.compile(\n        m_gen_optimizer = monet_generator_optimizer,\n        p_gen_optimizer = photo_generator_optimizer,\n        m_disc_optimizer = monet_discriminator_optimizer,\n        p_disc_optimizer = photo_discriminator_optimizer,\n        gen_loss_fn = generator_loss,\n        disc_loss_fn = discriminator_loss,\n        cycle_loss_fn = calc_cycle_loss,\n        identity_loss_fn = identity_loss\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cycle_gan_model.fit(\n    tf.data.Dataset.zip((monet_ds, photo_ds)),\n    epochs=25\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize our Monet-esque photos","metadata":{}},{"cell_type":"code","source":"_, ax = plt.subplots(5, 2, figsize=(12, 12))\nfor i, img in enumerate(photo_ds.take(5)):\n    prediction = monet_generator(img, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n\n    ax[i, 0].imshow(img)\n    ax[i, 1].imshow(prediction)\n    ax[i, 0].set_title(\"Input Photo\")\n    ax[i, 1].set_title(\"Monet-esque\")\n    ax[i, 0].axis(\"off\")\n    ax[i, 1].axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create submission file","metadata":{}},{"cell_type":"code","source":"import PIL\n! mkdir ../images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 1\nfor img in photo_ds:\n    prediction = monet_generator(img, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    im = PIL.Image.fromarray(prediction)\n    im.save(\"../images/\" + str(i) + \".jpg\")\n    i += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}