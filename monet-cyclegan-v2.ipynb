{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction and Setup\n\nThis notebook utilizes a CycleGAN architecture to add Monet-style to photos. For this tutorial, we will be using the TFRecord dataset. Import the following packages and change the accelerator to TPU.\n\nFor more information, check out [TensorFlow](https://www.tensorflow.org/tutorials/generative/cyclegan) and [Keras](https://keras.io/examples/generative/cyclegan/) CycleGAN documentation pages.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\nimport time\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport keras\nfrom keras import Sequential\nfrom keras.initializers import RandomNormal\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n    \nprint(tf.__version__)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-07-24T12:45:12.219903Z","iopub.execute_input":"2021-07-24T12:45:12.220226Z","iopub.status.idle":"2021-07-24T12:45:16.998702Z","shell.execute_reply.started":"2021-07-24T12:45:12.220195Z","shell.execute_reply":"2021-07-24T12:45:16.997698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install git+https://github.com/tensorflow/examples.git","metadata":{"execution":{"iopub.status.busy":"2021-07-11T19:26:22.829689Z","iopub.execute_input":"2021-07-11T19:26:22.829999Z","iopub.status.idle":"2021-07-11T19:26:25.51739Z","shell.execute_reply.started":"2021-07-11T19:26:22.82997Z","shell.execute_reply":"2021-07-11T19:26:25.516535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow_examples.models.pix2pix import pix2pix\n# from IPython.display import clear_output","metadata":{"execution":{"iopub.status.busy":"2021-07-18T16:36:14.068284Z","iopub.execute_input":"2021-07-18T16:36:14.068879Z","iopub.status.idle":"2021-07-18T16:36:14.072868Z","shell.execute_reply.started":"2021-07-18T16:36:14.068837Z","shell.execute_reply":"2021-07-18T16:36:14.071758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load in the data\n\nWe want to keep our photo dataset and our Monet dataset separate. First, load in the filenames of the TFRecords.","metadata":{}},{"cell_type":"code","source":"MONET_FILENAMES = tf.io.gfile.glob('../input/monet-gan-getting-started/monet_tfrec/*.tfrec')\nprint('Monet TFRecord Files:', len(MONET_FILENAMES))\n\nPHOTO_FILENAMES = tf.io.gfile.glob('../input/monet-gan-getting-started/photo_tfrec/*.tfrec')\nprint('Photo TFRecord Files:', len(PHOTO_FILENAMES))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:45:20.640583Z","iopub.execute_input":"2021-07-24T12:45:20.640934Z","iopub.status.idle":"2021-07-24T12:45:20.663273Z","shell.execute_reply.started":"2021-07-24T12:45:20.640903Z","shell.execute_reply":"2021-07-24T12:45:20.662162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the images for the competition are already sized to 256x256. As these images are RGB images, set the channel to 3. Additionally, we need to scale the images to a [-1, 1] scale. Because we are building a generative model, we don't need the labels or the image id so we'll only return the image from the TFRecord.","metadata":{}},{"cell_type":"code","source":"IMAGE_SIZE = [256, 256]\n\ndef decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = (tf.cast(image, tf.float32) / 127.5) - 1\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\n\ndef read_tfrecord(example):\n    tfrecord_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    return image","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:45:22.471221Z","iopub.execute_input":"2021-07-24T12:45:22.471552Z","iopub.status.idle":"2021-07-24T12:45:22.479827Z","shell.execute_reply.started":"2021-07-24T12:45:22.471518Z","shell.execute_reply":"2021-07-24T12:45:22.47862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the function to extract the image from the files.","metadata":{}},{"cell_type":"code","source":"def load_dataset(filenames, labeled=True, ordered=False):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:45:23.618983Z","iopub.execute_input":"2021-07-24T12:45:23.619328Z","iopub.status.idle":"2021-07-24T12:45:23.624384Z","shell.execute_reply.started":"2021-07-24T12:45:23.619296Z","shell.execute_reply":"2021-07-24T12:45:23.623582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's load in our datasets.","metadata":{}},{"cell_type":"code","source":"monet_ds = load_dataset(MONET_FILENAMES, labeled=True).batch(1)\nphoto_ds = load_dataset(PHOTO_FILENAMES, labeled=True).batch(1)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:45:24.940995Z","iopub.execute_input":"2021-07-24T12:45:24.941307Z","iopub.status.idle":"2021-07-24T12:45:27.1874Z","shell.execute_reply.started":"2021-07-24T12:45:24.941278Z","shell.execute_reply":"2021-07-24T12:45:27.186653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_monet = next(iter(monet_ds))\nexample_photo = next(iter(photo_ds))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:45:27.189267Z","iopub.execute_input":"2021-07-24T12:45:27.189704Z","iopub.status.idle":"2021-07-24T12:45:27.325132Z","shell.execute_reply.started":"2021-07-24T12:45:27.189661Z","shell.execute_reply":"2021-07-24T12:45:27.324386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's  visualize a photo example and a Monet example.","metadata":{}},{"cell_type":"code","source":"plt.subplot(121)\nplt.title('Photo')\nplt.imshow(example_photo[0] * 0.5 + 0.5)\n\nplt.subplot(122)\nplt.title('Monet')\nplt.imshow(example_monet[0] * 0.5 + 0.5)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:45:27.326512Z","iopub.execute_input":"2021-07-24T12:45:27.326851Z","iopub.status.idle":"2021-07-24T12:45:28.101104Z","shell.execute_reply.started":"2021-07-24T12:45:27.326814Z","shell.execute_reply":"2021-07-24T12:45:28.100215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pix2Pix Model\nThis is focused on the pre trained model by Tensorflow using the paper Pix2Pix. Based on the Tensorflow guide\n## Preprocessing","metadata":{}},{"cell_type":"code","source":"# important constants\nBUFFER_SIZE = 1000\nBATCH_SIZE = 1\nIMG_WIDTH = 256\nIMG_HEIGHT = 256","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:45:29.000111Z","iopub.execute_input":"2021-07-24T12:45:29.000436Z","iopub.status.idle":"2021-07-24T12:45:29.005271Z","shell.execute_reply.started":"2021-07-24T12:45:29.000405Z","shell.execute_reply":"2021-07-24T12:45:29.003915Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing Functions\n\ndef random_crop(image):\n  cropped_image = tf.image.random_crop(\n      image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n\n  return cropped_image\n\n# normalizing the images to [-1, 1]\ndef normalize(image):\n  image = tf.cast(image, tf.float32)\n  image = (image / 127.5) - 1\n  return image\n\ndef random_jitter(image):\n  # resizing to 286 x 286 x 3\n  image = tf.image.resize(image, [286, 286],\n                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n  # randomly cropping to 256 x 256 x 3\n  image = random_crop(image)\n\n  # random mirroring\n  image = tf.image.random_flip_left_right(image)\n\n  return image\n\ndef preprocess_image_train(image, label):\n  image = random_jitter(image)\n  image = normalize(image)\n  return image\n\ndef preprocess_image_test(image, label):\n  image = normalize(image)\n  return image","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:09:41.73653Z","iopub.execute_input":"2021-07-08T12:09:41.736852Z","iopub.status.idle":"2021-07-08T12:09:41.746074Z","shell.execute_reply.started":"2021-07-08T12:09:41.736821Z","shell.execute_reply":"2021-07-08T12:09:41.745238Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating generators and discriminators","metadata":{}},{"cell_type":"code","source":"OUTPUT_CHANNELS = 3\n\ngenerator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\ngenerator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n\ndiscriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\ndiscriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:19:59.383055Z","iopub.execute_input":"2021-06-15T17:19:59.383424Z","iopub.status.idle":"2021-06-15T17:20:01.091562Z","shell.execute_reply.started":"2021-06-15T17:19:59.383389Z","shell.execute_reply":"2021-06-15T17:20:01.090748Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_monet = generator_g(example_photo)\nto_photo = generator_f(example_monet)\nplt.figure(figsize=(8, 8))\ncontrast = 8\n\nimgs = [example_photo, to_monet, example_monet, to_photo]\ntitle = ['Photo', 'To Monet', 'Monet', 'To Photo']\n\nfor i in range(len(imgs)):\n  plt.subplot(2, 2, i+1)\n  plt.title(title[i])\n  if i % 2 == 0:\n    plt.imshow(imgs[i][0] * 0.5 + 0.5)\n  else:\n    plt.imshow(imgs[i][0] * 0.5 * contrast + 0.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:20:03.367818Z","iopub.execute_input":"2021-06-15T17:20:03.368141Z","iopub.status.idle":"2021-06-15T17:20:06.589627Z","shell.execute_reply.started":"2021-06-15T17:20:03.36811Z","shell.execute_reply":"2021-06-15T17:20:06.588767Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss Function","metadata":{}},{"cell_type":"code","source":"LAMBDA = 10\nloss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:20:10.346787Z","iopub.execute_input":"2021-06-15T17:20:10.347118Z","iopub.status.idle":"2021-06-15T17:20:10.353488Z","shell.execute_reply.started":"2021-06-15T17:20:10.347085Z","shell.execute_reply":"2021-06-15T17:20:10.352513Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def discriminator_loss(real, generated):\n  real_loss = loss_obj(tf.ones_like(real), real)\n\n  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n\n  total_disc_loss = real_loss + generated_loss\n\n  return total_disc_loss * 0.5","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:20:12.57093Z","iopub.execute_input":"2021-06-15T17:20:12.571247Z","iopub.status.idle":"2021-06-15T17:20:12.576211Z","shell.execute_reply.started":"2021-06-15T17:20:12.571217Z","shell.execute_reply":"2021-06-15T17:20:12.575283Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generator_loss(generated):\n  return loss_obj(tf.ones_like(generated), generated)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:20:15.608957Z","iopub.execute_input":"2021-06-15T17:20:15.609399Z","iopub.status.idle":"2021-06-15T17:20:15.617205Z","shell.execute_reply.started":"2021-06-15T17:20:15.609357Z","shell.execute_reply":"2021-06-15T17:20:15.616275Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_cycle_loss(real_image, cycled_image):\n  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n  \n  return LAMBDA * loss1","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:20:21.46546Z","iopub.execute_input":"2021-06-15T17:20:21.465889Z","iopub.status.idle":"2021-06-15T17:20:21.470714Z","shell.execute_reply.started":"2021-06-15T17:20:21.465846Z","shell.execute_reply":"2021-06-15T17:20:21.469568Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def identity_loss(real_image, same_image):\n  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n  return LAMBDA * 0.5 * loss","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:20:22.535063Z","iopub.execute_input":"2021-06-15T17:20:22.53539Z","iopub.status.idle":"2021-06-15T17:20:22.54106Z","shell.execute_reply.started":"2021-06-15T17:20:22.535357Z","shell.execute_reply":"2021-06-15T17:20:22.540067Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ngenerator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\ndiscriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:20:24.220857Z","iopub.execute_input":"2021-06-15T17:20:24.22117Z","iopub.status.idle":"2021-06-15T17:20:24.227091Z","shell.execute_reply.started":"2021-06-15T17:20:24.22114Z","shell.execute_reply":"2021-06-15T17:20:24.226235Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"EPOCHS = 40\ndef generate_images(model, test_input):\n    prediction = model(test_input)\n\n#     showing the prediction vs the input\n#     plt.figure(figsize=(12, 12))\n\n#     display_list = [test_input[0], prediction[0]]\n#     title = ['Input Image', 'Predicted Image']\n\n#     for i in range(2):\n#         plt.subplot(1, 2, i+1)\n#         plt.title(title[i])\n#         # getting the pixel values between [0, 1] to plot it.\n#         plt.imshow(display_list[i] * 0.5 + 0.5)\n#         plt.axis('off')\n#     plt.show()\n    return prediction","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:20:45.590592Z","iopub.execute_input":"2021-06-15T17:20:45.590931Z","iopub.status.idle":"2021-06-15T17:20:45.597495Z","shell.execute_reply.started":"2021-06-15T17:20:45.5909Z","shell.execute_reply":"2021-06-15T17:20:45.59629Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(real_x, real_y):\n  # persistent is set to True because the tape is used more than\n  # once to calculate the gradients.\n  with tf.GradientTape(persistent=True) as tape:\n    # Generator G translates X -> Y\n    # Generator F translates Y -> X.\n\n    fake_y = generator_g(real_x, training=True)\n    cycled_x = generator_f(fake_y, training=True)\n\n    fake_x = generator_f(real_y, training=True)\n    cycled_y = generator_g(fake_x, training=True)\n\n    # same_x and same_y are used for identity loss.\n    same_x = generator_f(real_x, training=True)\n    same_y = generator_g(real_y, training=True)\n\n    disc_real_x = discriminator_x(real_x, training=True)\n    disc_real_y = discriminator_y(real_y, training=True)\n\n    disc_fake_x = discriminator_x(fake_x, training=True)\n    disc_fake_y = discriminator_y(fake_y, training=True)\n\n    # calculate the loss\n    gen_g_loss = generator_loss(disc_fake_y)\n    gen_f_loss = generator_loss(disc_fake_x)\n\n    total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n\n    # Total generator loss = adversarial loss + cycle loss\n    total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n    total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n\n    disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n    disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n\n  # Calculate the gradients for generator and discriminator\n  generator_g_gradients = tape.gradient(total_gen_g_loss, \n                                        generator_g.trainable_variables)\n  generator_f_gradients = tape.gradient(total_gen_f_loss, \n                                        generator_f.trainable_variables)\n\n  discriminator_x_gradients = tape.gradient(disc_x_loss, \n                                            discriminator_x.trainable_variables)\n  discriminator_y_gradients = tape.gradient(disc_y_loss, \n                                            discriminator_y.trainable_variables)\n\n  # Apply the gradients to the optimizer\n  generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n                                            generator_g.trainable_variables))\n\n  generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n                                            generator_f.trainable_variables))\n\n  discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n                                                discriminator_x.trainable_variables))\n\n  discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n                                                discriminator_y.trainable_variables))","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:20:47.084982Z","iopub.execute_input":"2021-06-15T17:20:47.085337Z","iopub.status.idle":"2021-06-15T17:20:47.099151Z","shell.execute_reply.started":"2021-06-15T17:20:47.085285Z","shell.execute_reply":"2021-06-15T17:20:47.098226Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_path = \"./checkpoints/train\"\n\nckpt = tf.train.Checkpoint(generator_g=generator_g,\n                           generator_f=generator_f,\n                           discriminator_x=discriminator_x,\n                           discriminator_y=discriminator_y,\n                           generator_g_optimizer=generator_g_optimizer,\n                           generator_f_optimizer=generator_f_optimizer,\n                           discriminator_x_optimizer=discriminator_x_optimizer,\n                           discriminator_y_optimizer=discriminator_y_optimizer)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n  ckpt.restore(ckpt_manager.latest_checkpoint)\n  print ('Latest checkpoint restored!!')","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:20:49.539865Z","iopub.execute_input":"2021-06-15T17:20:49.54019Z","iopub.status.idle":"2021-06-15T17:20:49.548012Z","shell.execute_reply.started":"2021-06-15T17:20:49.540159Z","shell.execute_reply":"2021-06-15T17:20:49.546927Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n  start = time.time()\n\n  n = 0\n  for image_x, image_y in tf.data.Dataset.zip((photo_ds, monet_ds)):\n    train_step(image_x, image_y)\n    if n % 10 == 0:\n      print ('.', end='')\n    n += 1\n\n  clear_output(wait=True)\n  # Using a consistent image (sample_horse) so that the progress of the model\n  # is clearly visible.\n  generate_images(generator_g, example_photo)\n\n  if (epoch + 1) % 5 == 0:\n    ckpt_save_path = ckpt_manager.save()\n    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n                                                         ckpt_save_path))\n\n  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n                                                      time.time()-start))","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:20:50.56875Z","iopub.execute_input":"2021-06-15T17:20:50.569067Z","iopub.status.idle":"2021-06-15T18:08:27.436515Z","shell.execute_reply.started":"2021-06-15T17:20:50.569039Z","shell.execute_reply":"2021-06-15T18:08:27.435472Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission File","metadata":{}},{"cell_type":"code","source":"import PIL\n! mkdir ../images","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:09:33.96808Z","iopub.execute_input":"2021-06-15T18:09:33.968448Z","iopub.status.idle":"2021-06-15T18:09:34.746889Z","shell.execute_reply.started":"2021-06-15T18:09:33.968411Z","shell.execute_reply":"2021-06-15T18:09:34.745886Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 1\nfor img in photo_ds:\n    prediction = generate_images(generator_g, img)[0]\n    tf.keras.preprocessing.image.save_img(\"../images/\" + str(i) + \".jpg\",prediction)\n    i += 1","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:09:38.413685Z","iopub.execute_input":"2021-06-15T18:09:38.414019Z","iopub.status.idle":"2021-06-15T18:13:14.717457Z","shell.execute_reply.started":"2021-06-15T18:09:38.413985Z","shell.execute_reply":"2021-06-15T18:13:14.716591Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Manually Built CycleGAN\nThe generator and discriminators are built from scratch inspired from the pix2pix models\n\n## Build the generator\n\nWe'll be using a UNET architecture for our CycleGAN. To build our generator, let's first define our `downsample` and `upsample` methods.\n\nThe `downsample`, as the name suggests, reduces the 2D dimensions, the width and height, of the image by the stride. The stride is the length of the step the filter takes. Since the stride is 2, the filter is applied to every other pixel, hence reducing the weight and height by 2.\n\nWe'll be using an instance normalization instead of batch normalization. As the instance normalization is not standard in the TensorFlow API, we'll use the layer from TensorFlow Add-ons.","metadata":{}},{"cell_type":"markdown","source":"`Upsample` does the opposite of downsample and increases the dimensions of the of the image. `Conv2DTranspose` does basically the opposite of a `Conv2D` layer.","metadata":{}},{"cell_type":"code","source":"def upsample(filters, size, apply_dropout=False):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    result = keras.Sequential()\n    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n                                      padding='same',\n                                      kernel_initializer=initializer,\n                                      use_bias=False))\n\n    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n\n    if apply_dropout:\n        result.add(layers.Dropout(0.5))\n\n    result.add(layers.ReLU())\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-06-16T18:33:26.914925Z","iopub.execute_input":"2021-06-16T18:33:26.915248Z","iopub.status.idle":"2021-06-16T18:33:26.923889Z","shell.execute_reply.started":"2021-06-16T18:33:26.915214Z","shell.execute_reply":"2021-06-16T18:33:26.922981Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resnet(filters):\n    f1, f2 = filters\n    initializer = tf.random_normal_initializer(0., 0.02)\n\n    out_res_1 = layers.Conv2D(f1, 3, strides=1, padding='same', kernel_initializer=initializer) \n    out_res_2 = layers.Conv2D(f2, 3, strides=1, padding='same', kernel_initializer=initializer) \n\n    return (out_res_2 + input_res)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T16:32:51.515171Z","iopub.execute_input":"2021-06-17T16:32:51.51552Z","iopub.status.idle":"2021-06-17T16:32:51.522604Z","shell.execute_reply.started":"2021-06-17T16:32:51.515485Z","shell.execute_reply":"2021-06-17T16:32:51.521393Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's build our generator!\n\nThe generator first downsamples the input image and then upsample while establishing long skip connections. Skip connections are a way to help bypass the vanishing gradient problem by concatenating the output of a layer to multiple layers instead of only one. Here we concatenate the output of the downsample layer to the upsample layer in a symmetrical fashion.","metadata":{}},{"cell_type":"code","source":"gen_filters = 64","metadata":{"execution":{"iopub.status.busy":"2021-06-22T13:53:41.592157Z","iopub.execute_input":"2021-06-22T13:53:41.592594Z","iopub.status.idle":"2021-06-22T13:53:41.598069Z","shell.execute_reply.started":"2021-06-22T13:53:41.592562Z","shell.execute_reply":"2021-06-22T13:53:41.596471Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Generator():\n    model = Sequential()\n    \n    model.add(layers.Dense(256, input_shape=(256,256,3)))\n    model.add(layers.LeakyReLU(0.2))\n    \n    model.add(layers.Dense(512))\n    model.add(layers.Conv2DTranspose(64, 64, activation='relu'))\n    model.add(layers.LeakyReLU(0.2))\n    \n    model.add(layers.Dense(1024))\n    model.add(layers.LeakyReLU(0.2))\n        \n    model.add(layers.Dense(3, activation='tanh'))\n        \n    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(2e-4, beta_1=0.5))\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-04T19:50:53.208165Z","iopub.execute_input":"2021-07-04T19:50:53.208551Z","iopub.status.idle":"2021-07-04T19:50:53.217654Z","shell.execute_reply.started":"2021-07-04T19:50:53.208517Z","shell.execute_reply":"2021-07-04T19:50:53.216268Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Discriminator():\n    \n    model = Sequential()\n    \n    model.add(layers.Dense(units=1024, input_shape=(256,256,3)))    \n    model.add(layers.LeakyReLU(0.2))\n    model.add(layers.Dropout(0.3))\n       \n    model.add(layers.Dense(units=512))\n    model.add(layers.Conv2D(64, 64, activation='relu'))\n    model.add(layers.LeakyReLU(0.2))\n    model.add(layers.Dropout(0.3))\n    \n    \n    model.add(layers.Dense(units=256))\n    model.add(layers.LeakyReLU(0.2))    \n\n    \n    model.add(layers.Dense(units=1, activation='sigmoid'))\n    \n    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5))\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-04T19:51:00.724416Z","iopub.execute_input":"2021-07-04T19:51:00.724741Z","iopub.status.idle":"2021-07-04T19:51:00.736523Z","shell.execute_reply.started":"2021-07-04T19:51:00.724712Z","shell.execute_reply":"2021-07-04T19:51:00.735568Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Discriminator(in_shape=(256,256,3)):\n    # weight initialization\n    init = RandomNormal(stddev=0.02)\n    # define model\n    model = Sequential()\n    # downsample to 128x128\n    model.add(layers.Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init, input_shape=in_shape))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    # downsample to 64x64\n    model.add(layers.Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    # downsample to 32x32\n    model.add(layers.Conv2D(32, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    # downsample to 16x16\n    model.add(layers.Conv2D(32, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    # classifier\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1, activation='sigmoid'))\n    # compile model\n    opt = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:45:41.918989Z","iopub.execute_input":"2021-07-24T12:45:41.91933Z","iopub.status.idle":"2021-07-24T12:45:41.935232Z","shell.execute_reply.started":"2021-07-24T12:45:41.919299Z","shell.execute_reply":"2021-07-24T12:45:41.934277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Generator(input_shape=(256,256,3)):\n    # weight initialization\n    init = RandomNormal(stddev=0.02)\n    # define model\n    model = Sequential()\n    # foundation for 256x256 image\n    model.add(layers.Dense(64, kernel_initializer=init, input_shape=input_shape))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    # downsample to 128x128\n    model.add(layers.Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    # downsample to 64x64\n    model.add(layers.Conv2D(32, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dense(units=64*64))\n    # downsample to 32x32\n    model.add(layers.Conv2D(16, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    # upsample to 64x64\n    model.add(layers.Conv2DTranspose(32, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    # upsample to 128x128\n    model.add(layers.Conv2DTranspose(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    # upsample to 256x256\n    model.add(layers.Conv2DTranspose(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    # output 256x256x3\n    model.add(layers.Conv2D(3, (7,7), activation='tanh', padding='same', kernel_initializer=init))\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:46:34.431558Z","iopub.execute_input":"2021-07-24T12:46:34.43189Z","iopub.status.idle":"2021-07-24T12:46:34.449239Z","shell.execute_reply.started":"2021-07-24T12:46:34.431859Z","shell.execute_reply":"2021-07-24T12:46:34.4484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def GAN(dis, gen):\n    x = layers.Input(shape=[256,256,3])\n    \n    generated_image = gen(x)\n    gan_decision = dis(generated_image)\n    \n    gan = Model(inputs = x, outputs = gan_decision)\n    \n    gan.compile(loss='binary_crossentropy', optimizer='adam')\n    \n    return gan","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:45:58.118385Z","iopub.execute_input":"2021-07-24T12:45:58.118732Z","iopub.status.idle":"2021-07-24T12:45:58.12662Z","shell.execute_reply.started":"2021-07-24T12:45:58.118701Z","shell.execute_reply":"2021-07-24T12:45:58.125585Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Generator():\n    inputs = layers.Input(shape=[256,256,3])\n    initializer = tf.random_normal_initializer(0., 0.02)\n    skips = []\n    \n#     # bs = batch size\n#     down_stack = [\n#         downsample(64, 4, apply_instancenorm=False), # (bs, 128, 128, 64)\n#         downsample(128, 4), # (bs, 64, 64, 128)\n#         downsample(256, 4), # (bs, 32, 32, 256)\n#         downsample(512, 4), # (bs, 16, 16, 512)\n#         downsample(512, 4), # (bs, 8, 8, 512)\n#         downsample(512, 4), # (bs, 4, 4, 512)\n#         downsample(512, 4), # (bs, 2, 2, 512)\n#         downsample(512, 4), # (bs, 1, 1, 512)\n#     ]\n\n#     up_stack = [\n#         upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n#         upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n#         upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n#         upsample(512, 4), # (bs, 16, 16, 1024)\n#         upsample(256, 4), # (bs, 32, 32, 512)\n#         upsample(128, 4), # (bs, 64, 64, 256)\n#         upsample(64, 4), # (bs, 128, 128, 128)\n#     ]\n\n    x = inputs\n    model = Sequential()\n    model.add(layers.Conv2D(64, 7, strides=1, padding='same', kernel_initializer=initializer, use_bias=False))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(128, 3, strides=1, padding='same', kernel_initializer=initializer))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(256, 3, strides=1, padding='same', kernel_initializer=initializer))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    \n#     for i in range(5):\n#         model.add(resnet((64, 256)))\n    \n    model.add(layers.Conv2DTranspose(128, 7, strides=1, padding='same', kernel_initializer=initializer))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(64, 7, strides=1, padding='same', kernel_initializer=initializer))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(3, 4, strides=1, padding='same', kernel_initializer=initializer))\n    model.add(layers.LeakyReLU(alpha=0.2))\n        \n#     convStack = [\n#         layers.Conv2D(64, 7, strides=1, padding='same', kernel_initializer=initializer, use_bias=False),\n#         layers.Conv2D(128, 3, strides=2, padding='same', kernel_initializer=initializer, use_bias=False),\n#         layers.Conv2D(256, 3, strides=2, padding='same', kernel_initializer=initializer, use_bias=False)\n#     ]\n    \n#     for conv in convStack:\n#         x = conv(x)\n#         skips.append(x)\n    \n#     for i in range(4):\n#         x = resnet(x, (64, 256))\n        \n#     skips = reversed(skips[:-1])\n    \n#     deconvStack = [\n#         layers.Conv2DTranspose(64, 3, strides=2, padding='same',kernel_initializer=initializer),\n# #         layers.Conv2DTranspose(64, 3, strides=2, padding='same',kernel_initializer=initializer)\n#     ]\n    \n#     for deconv, skip in zip(deconvStack, skips):\n#         x = deconv(x)\n#         x = layers.Concatenate()([x, skip])\n    \n       \n#     last = layers.Conv2DTranspose(3, 4,\n#                                   strides=2,\n#                                   padding='same',\n#                                   kernel_initializer=initializer,\n#                                   activation='tanh', use_bias=False)\n    \n   \n#     print(x.shape)\n#     # Downsampling through the model\n#     skips = []\n#     for down in down_stack:\n#         x = down(x)\n#         skips.append(x)\n\n#     skips = reversed(skips[:-1])\n\n#     # Upsampling and establishing the skip connections\n#     for up, skip in zip(up_stack, skips):\n#         x = up(x)\n#         x = layers.Concatenate()([x, skip])\n\n#     x = last(x)\n    \n#     print(x.shape)\n\n#     return keras.Model(inputs=inputs, outputs=x)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:45:59.363749Z","iopub.execute_input":"2021-07-24T12:45:59.364141Z","iopub.status.idle":"2021-07-24T12:45:59.380173Z","shell.execute_reply.started":"2021-07-24T12:45:59.364107Z","shell.execute_reply":"2021-07-24T12:45:59.378903Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build the discriminator\n\nThe discriminator takes in the input image and classifies it as real or fake (generated). Instead of outputing a single node, the discriminator outputs a smaller 2D image with higher pixel values indicating a real classification and lower values indicating a fake classification.","metadata":{}},{"cell_type":"code","source":"def Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n#     inp = layers.Input(shape=[256, 256, 3], name='input_image')\n\n#     x = inp\n    \n#     x = layers.Conv2D(64, 4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False) (x)\n#     x = layers.Conv2D(64*2, 4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False) (x)\n#     x = layers.Conv2D(64*4, 4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False) (x)\n#     x = layers.Conv2D(64*8, 4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False) (x)\n#     last = layers.Conv2D(1, 4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False) (x)\n    \n    model = Sequential()\n    model.add(layers.Conv2D(64, 4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n    model.add(layers.Conv2D(64*2, 4, strides=2, padding='same', kernel_initializer=initializer))\n    model.add(layers.Conv2D(64*4, 4, strides=2, padding='same', kernel_initializer=initializer))\n    \n    model.add(layers.ZeroPadding2D())\n    \n    model.add(layers.Conv2D(64*8, 4, strides=2, padding='same', kernel_initializer=initializer))\n    model.add(layers.LeakyReLU())\n    \n    model.add(layers.Conv2D(1, 4, strides=1, padding='same', kernel_initializer=initializer))\n\n#     down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n#     down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n#     down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n\n#     zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n#     conv = layers.Conv2D(512, 4, strides=1,\n#                          kernel_initializer=initializer,\n#                          use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n\n#     norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n\n#     leaky_relu = layers.LeakyReLU()(norm1)\n\n#     zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n\n#     last = layers.Conv2D(1, 4, strides=1,\n#                          kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n\n#     return tf.keras.Model(inputs=inp, outputs=last)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-28T18:12:18.903552Z","iopub.execute_input":"2021-06-28T18:12:18.903863Z","iopub.status.idle":"2021-06-28T18:12:18.913796Z","shell.execute_reply.started":"2021-06-28T18:12:18.903833Z","shell.execute_reply":"2021-06-28T18:12:18.912933Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmonet_generator = Generator() # transforms photos to Monet-esque paintings\nphoto_generator = Generator() # transforms Monet paintings to be more like photos\n\nmonet_discriminator = Discriminator() # differentiates real Monet paintings and generated Monet paintings\nphoto_discriminator = Discriminator() # differentiates real photos and generated photos","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:46:39.096399Z","iopub.execute_input":"2021-07-24T12:46:39.096801Z","iopub.status.idle":"2021-07-24T12:46:40.206911Z","shell.execute_reply.started":"2021-07-24T12:46:39.09677Z","shell.execute_reply":"2021-07-24T12:46:40.20607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since our generators are not trained yet, the generated Monet-esque photo does not show what is expected at this point.","metadata":{}},{"cell_type":"code","source":"monet_generator.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:46:42.251255Z","iopub.execute_input":"2021-07-24T12:46:42.251589Z","iopub.status.idle":"2021-07-24T12:46:42.26818Z","shell.execute_reply.started":"2021-07-24T12:46:42.251558Z","shell.execute_reply":"2021-07-24T12:46:42.266946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monet_discriminator.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:46:43.392439Z","iopub.execute_input":"2021-07-24T12:46:43.392769Z","iopub.status.idle":"2021-07-24T12:46:43.407206Z","shell.execute_reply.started":"2021-07-24T12:46:43.392739Z","shell.execute_reply":"2021-07-24T12:46:43.406249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_monet = monet_generator(example_photo)\nto_monet.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:46:44.529506Z","iopub.execute_input":"2021-07-24T12:46:44.529978Z","iopub.status.idle":"2021-07-24T12:46:44.668568Z","shell.execute_reply.started":"2021-07-24T12:46:44.529932Z","shell.execute_reply":"2021-07-24T12:46:44.667639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_monet = monet_generator(example_photo)\nto_photo = photo_generator(to_monet)\nplt.subplot(1, 2, 1)\nplt.title(\"Original Photo\")\nplt.imshow(example_photo[0] * 0.5 + 0.5)\n\nplt.subplot(1, 2, 2)\nplt.title(\"Monet-esque Photo\")\nplt.imshow(to_monet[0] * 0.5 + 0.5)\nplt.show()\n\nplt.subplot(1, 2, 2)\nplt.title(\"Photo of Monet\")\nplt.imshow(to_photo[0] * 0.5 + 0.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:46:45.889424Z","iopub.execute_input":"2021-07-24T12:46:45.889937Z","iopub.status.idle":"2021-07-24T12:46:46.288365Z","shell.execute_reply.started":"2021-07-24T12:46:45.889899Z","shell.execute_reply":"2021-07-24T12:46:46.287522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build the CycleGAN model\n\nWe will subclass a `tf.keras.Model` so that we can run `fit()` later to train our model. During the training step, the model transforms a photo to a Monet painting and then back to a photo. The difference between the original photo and the twice-transformed photo is the cycle-consistency loss. We want the original photo and the twice-transformed photo to be similar to one another.\n\nThe losses are defined in the next section.","metadata":{}},{"cell_type":"code","source":"class CycleGan(keras.Model):\n    def __init__(\n        self,\n        monet_generator,\n        photo_generator,\n        monet_discriminator,\n        photo_discriminator,\n        lambda_cycle=10,\n    ):\n        super(CycleGan, self).__init__()\n        self.m_gen = monet_generator\n        self.p_gen = photo_generator\n        self.m_disc = monet_discriminator\n        self.p_disc = photo_discriminator\n        self.lambda_cycle = lambda_cycle\n        \n    def compile(\n        self,\n        m_gen_optimizer,\n        p_gen_optimizer,\n        m_disc_optimizer,\n        p_disc_optimizer,\n        gen_loss_fn,\n        disc_loss_fn,\n        cycle_loss_fn,\n        identity_loss_fn\n    ):\n        super(CycleGan, self).compile()\n        self.m_gen_optimizer = m_gen_optimizer\n        self.p_gen_optimizer = p_gen_optimizer\n        self.m_disc_optimizer = m_disc_optimizer\n        self.p_disc_optimizer = p_disc_optimizer\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n        \n    def train_step(self, batch_data):\n        real_monet, real_photo = batch_data\n        \n        \n        with tf.GradientTape(persistent=True) as tape:\n            # photo to monet back to photo\n            fake_monet = self.m_gen(real_photo, training=True)\n            cycled_photo = self.p_gen(fake_monet, training=True)\n\n            # monet to photo back to monet\n            fake_photo = self.p_gen(real_monet, training=True)\n            cycled_monet = self.m_gen(fake_photo, training=True)\n\n            # generating itself\n            same_monet = self.m_gen(real_monet, training=True)\n            same_photo = self.p_gen(real_photo, training=True)\n\n            # discriminator used to check, inputing real images\n            disc_real_monet = self.m_disc(real_monet, training=True)\n            disc_real_photo = self.p_disc(real_photo, training=True)\n\n            # discriminator used to check, inputing fake images\n            disc_fake_monet = self.m_disc(fake_monet, training=True)\n            disc_fake_photo = self.p_disc(fake_photo, training=True)\n            \n            print(real_monet.shape)\n            print(cycled_monet.shape)\n            print(real_photo.shape)\n            print(cycled_photo.shape)\n\n            # evaluates generator loss\n            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n\n            \n            # evaluates total cycle consistency loss\n            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n\n            # evaluates total generator loss\n            total_monet_gen_loss = monet_gen_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle) + total_cycle_loss\n            total_photo_gen_loss = photo_gen_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle) + total_cycle_loss\n\n            # evaluates discriminator loss\n            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n\n        # Calculate the gradients for generator and discriminator\n        monet_generator_gradients = tape.gradient(total_monet_gen_loss,\n                                                  self.m_gen.trainable_variables)\n        photo_generator_gradients = tape.gradient(total_photo_gen_loss,\n                                                  self.p_gen.trainable_variables)\n\n        monet_discriminator_gradients = tape.gradient(monet_disc_loss,\n                                                      self.m_disc.trainable_variables)\n        photo_discriminator_gradients = tape.gradient(photo_disc_loss,\n                                                      self.p_disc.trainable_variables)\n\n        # Apply the gradients to the optimizer\n        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients,\n                                                 self.m_gen.trainable_variables))\n\n        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients,\n                                                 self.p_gen.trainable_variables))\n\n        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients,\n                                                  self.m_disc.trainable_variables))\n\n        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,\n                                                  self.p_disc.trainable_variables))\n        \n        return {\n            \"monet_gen_loss\": total_monet_gen_loss,\n            \"photo_gen_loss\": total_photo_gen_loss,\n            \"monet_disc_loss\": monet_disc_loss,\n            \"photo_disc_loss\": photo_disc_loss\n        }","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:46:48.610518Z","iopub.execute_input":"2021-07-24T12:46:48.610841Z","iopub.status.idle":"2021-07-24T12:46:48.635346Z","shell.execute_reply.started":"2021-07-24T12:46:48.610811Z","shell.execute_reply":"2021-07-24T12:46:48.634555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define loss functions\n\nThe discriminator loss function below compares real images to a matrix of 1s and fake images to a matrix of 0s. The perfect discriminator will output all 1s for real images and all 0s for fake images. The discriminator loss outputs the average of the real and generated loss.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    def discriminator_loss(real, generated):\n        real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n\n        generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n\n        total_disc_loss = real_loss + generated_loss\n\n        return total_disc_loss * 0.5","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:46:50.453086Z","iopub.execute_input":"2021-07-24T12:46:50.453398Z","iopub.status.idle":"2021-07-24T12:46:50.461186Z","shell.execute_reply.started":"2021-07-24T12:46:50.453368Z","shell.execute_reply":"2021-07-24T12:46:50.460045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The generator wants to fool the discriminator into thinking the generated image is real. The perfect generator will have the discriminator output only 1s. Thus, it compares the generated image to a matrix of 1s to find the loss.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    def generator_loss(generated):\n        return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:46:54.326841Z","iopub.execute_input":"2021-07-24T12:46:54.327199Z","iopub.status.idle":"2021-07-24T12:46:54.333319Z","shell.execute_reply.started":"2021-07-24T12:46:54.327166Z","shell.execute_reply":"2021-07-24T12:46:54.332435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We want our original photo and the twice transformed photo to be similar to one another. Thus, we can calculate the cycle consistency loss be finding the average of their difference.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n        loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n\n        return LAMBDA * loss1","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:46:56.263908Z","iopub.execute_input":"2021-07-24T12:46:56.264226Z","iopub.status.idle":"2021-07-24T12:46:56.269067Z","shell.execute_reply.started":"2021-07-24T12:46:56.264195Z","shell.execute_reply":"2021-07-24T12:46:56.268225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The identity loss compares the image with its generator (i.e. photo with photo generator). If given a photo as input, we want it to generate the same image as the image was originally a photo. The identity loss compares the input with the output of the generator.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    def identity_loss(real_image, same_image, LAMBDA):\n        loss = tf.reduce_mean(tf.abs(real_image - same_image))\n        return LAMBDA * 0.5 * loss","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:46:57.878105Z","iopub.execute_input":"2021-07-24T12:46:57.878466Z","iopub.status.idle":"2021-07-24T12:46:57.883601Z","shell.execute_reply.started":"2021-07-24T12:46:57.878437Z","shell.execute_reply":"2021-07-24T12:46:57.88245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the CycleGAN\n\nLet's compile our model. Since we used `tf.keras.Model` to build our CycleGAN, we can just ude the `fit` function to train our model.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n    monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:46:59.563816Z","iopub.execute_input":"2021-07-24T12:46:59.564145Z","iopub.status.idle":"2021-07-24T12:46:59.571896Z","shell.execute_reply.started":"2021-07-24T12:46:59.564112Z","shell.execute_reply":"2021-07-24T12:46:59.570542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    cycle_gan_model = CycleGan(\n        monet_generator, photo_generator, monet_discriminator, photo_discriminator\n    )\n\n    cycle_gan_model.compile(\n        m_gen_optimizer = monet_generator_optimizer,\n        p_gen_optimizer = photo_generator_optimizer,\n        m_disc_optimizer = monet_discriminator_optimizer,\n        p_disc_optimizer = photo_discriminator_optimizer,\n        gen_loss_fn = generator_loss,\n        disc_loss_fn = discriminator_loss,\n        cycle_loss_fn = calc_cycle_loss,\n        identity_loss_fn = identity_loss\n    )","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:47:00.481355Z","iopub.execute_input":"2021-07-24T12:47:00.481721Z","iopub.status.idle":"2021-07-24T12:47:00.505832Z","shell.execute_reply.started":"2021-07-24T12:47:00.48167Z","shell.execute_reply":"2021-07-24T12:47:00.504898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cycle_gan_model.fit(\n    tf.data.Dataset.zip((monet_ds, photo_ds)),\n    epochs=40\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T12:47:05.135635Z","iopub.execute_input":"2021-07-24T12:47:05.135962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize our Monet-esque photos","metadata":{}},{"cell_type":"code","source":"_, ax = plt.subplots(5, 2, figsize=(12, 12))\nfor i, img in enumerate(photo_ds.take(5)):\n    prediction = monet_generator(img, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n\n    ax[i, 0].imshow(img)\n    ax[i, 1].imshow(prediction)\n    ax[i, 0].set_title(\"Input Photo\")\n    ax[i, 1].set_title(\"Monet-esque\")\n    ax[i, 0].axis(\"off\")\n    ax[i, 1].axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create submission file","metadata":{}},{"cell_type":"code","source":"import PIL\n! mkdir ../images","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:10:12.209437Z","iopub.execute_input":"2021-07-16T21:10:12.209812Z","iopub.status.idle":"2021-07-16T21:10:12.867364Z","shell.execute_reply.started":"2021-07-16T21:10:12.209766Z","shell.execute_reply":"2021-07-16T21:10:12.8664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 1\nfor img in photo_ds:\n    prediction = monet_generator(img, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    im = PIL.Image.fromarray(prediction)\n    im.save(\"../images/\" + str(i) + \".jpg\")\n    i += 1","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:10:13.808603Z","iopub.execute_input":"2021-07-16T21:10:13.808966Z","iopub.status.idle":"2021-07-16T21:12:26.416228Z","shell.execute_reply.started":"2021-07-16T21:10:13.808929Z","shell.execute_reply":"2021-07-16T21:12:26.413176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")","metadata":{"execution":{"iopub.status.busy":"2021-07-16T21:12:32.388245Z","iopub.execute_input":"2021-07-16T21:12:32.388684Z","iopub.status.idle":"2021-07-16T21:12:35.522837Z","shell.execute_reply.started":"2021-07-16T21:12:32.388642Z","shell.execute_reply":"2021-07-16T21:12:35.521924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}